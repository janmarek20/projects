# -*- coding: utf-8 -*-
"""task_prediction_model_jm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jDDYc5T2uUWOQrJKSRIXrMLiNRl847XR
"""

import pandas as pd
import numpy as np

"""Import data - store_sales_per_category.csv"""

df_store_sales_og = pd.read_csv('store_sales_per_category.csv', sep=';')
df_store_sales = df_store_sales_og.copy()
df_store_sales.head(30)

pd.set_option('display.float_format', '{:.2f}'.format)

df_store_sales.describe()

df_store_sales.info()

df_store_sales.isnull().sum()

"""Data preparation - store_sales_per_category.csv"""

df_store_sales['general_volume'] = df_store_sales[['Vodka', 'Tequila',
                      'Whiskey', 'Other', 'Gin', 'Brandy', 'Rum']].sum(axis=1)

df_store_sales = df_store_sales.sort_values(by=['store_id', 'year', 'week']).\
                                                        reset_index(drop=True)

df_store_sales.head(30)

df = pd.DataFrame(df_store_sales)

# Number of days from 'year' and 'week' columns
df_store_sales['date'] = pd.to_datetime('2012-01-01') + \
pd.to_timedelta((df_store_sales['week'] - 1) * 7, unit='D')

df_store_sales['days_since_start'] = df_store_sales.apply(lambda row: (row['date'] - pd.to_datetime('2012-01-01')).days + (row['year'] - 2012) * 365, axis=1)

df_store_sales.head(30)

df_store_sales.drop('date', axis=1, inplace=True)

df_store_sales

"""Visualization of the relationship between the volume and the number of days"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

def volume_v_no_days(x):
  """
  The function returns a graph of the relationship between
  the volume and the number of days for the given store_id.
  """

  filtered_df = df_store_sales[df_store_sales['store_id'] == x]

  plt.plot(filtered_df['days_since_start'], filtered_df['general_volume'])
  plt.xlabel('Number of days')
  plt.ylabel('General Volume')
  plt.title(f'General volume change for store_id={x}')
  plt.grid(True)


  plt.show()

volume_v_no_days(115)

"""Extracting unique store identifiers to use for trend determination for each store ID"""

store_ids = df_store_sales['store_id']
store_ids = store_ids.unique()
len(store_ids)

for store_id in store_ids:
    store_data = df_store_sales[df_store_sales['store_id'] == store_id]
    if len(store_data) < 26:
        id_to_remove = np.where(store_ids == store_id)
        store_ids = np.delete(store_ids, id_to_remove)

df_store_sales

df_store_sales.info()

len(store_ids)

df_store_sales.describe().T

"""Prediction model - assessment of the volume trend - increase or decrease"""

from sklearn.linear_model import LinearRegression
#from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

trend_list=[]
store_id_list = []

for id in store_ids:
  store_data = df_store_sales[df_store_sales['store_id'] == id]

  # Data preparation
  X = store_data['days_since_start']
  y = store_data['general_volume']

  X = X.values.reshape(-1, 1)

  #poly = PolynomialFeatures(degree=2)

  #X_poly = poly.fit_transform(X)

  # Initialization of the model
  model = LinearRegression()
  model.fit(X, y)

  # Model-based predictions
  y_pred = model.predict(X)

  coef = model.coef_[0]
  # Slope coefficient (coef_)
  if coef > 0:
      credit = 1
  else:
      credit = 0
  trend_list.append(credit)
  store_id_list.append(id)

df_results = pd.DataFrame({
    'store_id': store_id_list,
    'trend': trend_list
})

df_results

"""Regression plot"""

def plot_regression(df_store_sales, store_id):
    """
    The function draws a regression plot for the given store_id.
    """

    store_data = df_store_sales[df_store_sales['store_id'] == store_id]

    X = store_data['days_since_start']
    y = store_data['general_volume']

    X = X.values.reshape(-1, 1)

    #poly = PolynomialFeatures(degree=2)
    #X_poly = poly.fit_transform(X)

    model = LinearRegression()
    model.fit(X, y)

    y_pred = model.predict(X)

    coef = model.coef_[0]

    # Draw a graph of real data and a regression line
    plt.scatter(X, y, label='Real data')
    plt.plot(X, y_pred, color='red', linewidth=2,
             label='Linear Regression')
    plt.xlabel('Number of days')
    plt.ylabel('General Volume')
    plt.title(f'Regression flow for store_id={store_id}')
    plt.legend()
    plt.show()

plot_regression(df_store_sales, 115)

df_results['trend'].sum()

len(store_ids)

df_store_sales

df_store_sales = df_store_sales.groupby(['store_id', 'year']).agg({
                              'Vodka': 'sum','Tequila': 'sum', 'Whiskey': 'sum',
                              'Other': 'sum', 'Gin': 'sum', 'Brandy': 'sum',
                              'Rum': 'sum', 'general_volume': 'sum',
                              'days_since_start': 'max'
                          }).reset_index()

df_store_sales.head(50)

df_store_sales.tail(50)

"""Linear regression model to predict the volume for the next year"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Linear Regression

training_data = df_store_sales[df_store_sales['year'] < 2018]


features = ["year", "Vodka", "Tequila", "Whiskey", "Other",
            "Gin", "Brandy", "Rum"]

# Training data
X = training_data[features]
y = training_data['general_volume']  # Zmienna zależna

# Model initialization
model = LinearRegression()

# Model training
model.fit(X, y)

# Prediction for next year
prediction_data = pd.DataFrame()

for store_id in df_store_sales['store_id'].unique():
    temp_df = pd.DataFrame({'store_id': [store_id], 'year': [2018]})
    for feature in features:
        temp_df[feature] = df_store_sales[df_store_sales['store_id'] ==\
                                          store_id][feature].mean()

    prediction_data = pd.concat([prediction_data, temp_df], ignore_index=True)

X_pred = prediction_data[features]


predictions = model.predict(X_pred)

# Summarizing predicted values ​​for the entire year 2018
result = prediction_data[['store_id']].copy()
result['predicted_general_volume_2018'] = predictions
result = result.groupby('store_id')['predicted_general_volume_2018'].sum().\
                                                                reset_index()

# Expected product volumes for 2018
product_predictions = model.coef_[1:] * X_pred.iloc[:, 1:]
product_predictions['store_id'] = prediction_data['store_id']
product_predictions = product_predictions.groupby('store_id').sum().\
                                                        reset_index()
product_predictions['store_id'] = result['store_id']


results = product_predictions.merge(result, on='store_id')

results.head(30)

df_store_2018 = results.merge(df_results,
                      left_on='store_id', right_on='store_id', how='outer')
df_store_2018

df_store_2018.info()

df_store_2018.dropna()

"""Import data - store_distances_anonymized.csv"""

df_store_distances_og = pd.read_csv('store_distances_anonymized.csv')
df_store_distances = df_store_distances_og.copy()
df_store_distances.head(30)

no_of_near_stores_1 = df_store_distances.groupby('store_id_1')['distance'].\
                                            agg(['sum', 'count']).reset_index()

no_of_near_stores_1.columns = ['store_id_1', 'total_distance_1',
                               'count_shops_1']

no_of_near_stores_1

no_of_near_stores_2 = df_store_distances.groupby('store_id_2')['distance'].\
                                            agg(['sum', 'count']).reset_index()

no_of_near_stores_2.columns = ['store_id_2', 'total_distance_2',
                               'count_shops_2']

no_of_near_stores_2

no_of_near_stores = no_of_near_stores_1.merge(no_of_near_stores_2,
                      left_on='store_id_1', right_on='store_id_2', how='outer')
no_of_near_stores

no_of_near_stores['store_id_1'].fillna(no_of_near_stores['store_id_2'],
                                       inplace=True)
no_of_near_stores

no_of_near_stores['total_distance'] = no_of_near_stores['total_distance_1'].\
                    add(no_of_near_stores['total_distance_2'], fill_value=0.0)

no_of_near_stores['number_of_near_shops'] = no_of_near_stores['count_shops_1'].\
                      add(no_of_near_stores['count_shops_2'], fill_value=0.0)

no_of_near_stores.head(14)

no_of_near_stores = no_of_near_stores.iloc[:, [0, 6, 7]]

no_of_near_stores.info()

no_of_near_stores['average_distance'] = no_of_near_stores['total_distance'] / \
                                      no_of_near_stores['number_of_near_shops']
no_of_near_stores.head(25)

no_of_near_stores.rename(columns={'store_id_1': 'store_id'}, inplace=True)
no_of_near_stores = no_of_near_stores.drop(['total_distance'], axis=1)
no_of_near_stores

no_of_near_stores.head(30)

"""Import data - gdata_anonymized.csv"""

df_og = pd.read_csv('gdata_anonymized.csv')
df_objects = df_og
df_objects.head(30)

df_objects = df_objects.drop(['churches', 'gym'], axis=1)

columns = [col.replace(' ', '_') for col in df_objects.columns]
columns

df_objects.columns = columns
df_objects

df_objects['positive_objects'] = df_objects['university_or_college'] + \
                                                    df_objects['stadium']

n_objects = df_objects['foodstores_or_supermarkets_or_gorceries']
restaurants = df_objects['restaurant']

df_objects['negative_objects'] = n_objects + restaurants
df_objects.head(30)

df_objects = df_objects.iloc[:, -3:]
df_objects

df_objects = df_objects.sort_values('store_id')

df_objects.head(30)

"""Dataframes connection"""

df_stores_data = pd.merge(df_store_2018, no_of_near_stores,
                      left_on=['store_id'], right_on=['store_id'], how='outer')
df_stores_data.head(40)

df_stores_data = pd.merge(df_stores_data, df_objects,
                          on='store_id', how='outer')
df_stores_data

df_stores_data = df_stores_data.sort_values('store_id')
df_stores_data

df_stores_data.info()

df_stores_data['number_of_near_shops'].fillna(0, inplace=True)
df_stores_data['average_distance'].fillna(0, inplace=True)

df_stores_data.info()

df_stores_data.dropna(inplace=True)

df_stores_data.info()

df_stores_data[['store_id', 'trend', 'number_of_near_shops']] = \
      df_stores_data[['store_id', 'trend', 'number_of_near_shops']]\
      .astype(int)
df_stores_data

df_stores_data.head(30)

df_stores = df_stores_data.copy()

df_stores.head(20)

df_stores = df_stores.iloc[:, [10, 12, 13]]
df_stores.head(20)

df_stores.head(20)

"""Clustering into groups - credit or no credit"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score,\
calinski_harabasz_score


X = df_stores

kmeans = KMeans(n_clusters=2, random_state=42)

kmeans.fit(X)

df_stores['cluster'] = kmeans.labels_

df_stores['credit'] = df_stores['cluster'].apply(lambda x: 1 if x == 0 else 0)

silhouette = silhouette_score(X, kmeans.labels_)
davies_bouldin = davies_bouldin_score(X, kmeans.labels_)
calinski_harabasz = calinski_harabasz_score(X, kmeans.labels_)

print("Silhouette Score:", silhouette)
print("Davies-Bouldin Score:", davies_bouldin)
print("Calinski-Harabasz Score:", calinski_harabasz)

df_stores = df_stores.iloc[:, [-2, -1]]
df_stores.head(20)

df_main = pd.concat([df_stores_data, df_stores], axis=1)
df_main

df_main.isnull().sum()

"""Conditions for obtaining a credit"""

def calculate_target(row):
    """
    The function calculates the value into a new target column
    for future classifications
    """
    if row['trend'] == 1 and row['credit'] == 1:
      return 1
    elif row['trend'] == 1 and row['predicted_general_volume_2018'] > 50000:
      return 1
    elif row['credit'] == 1 and row['predicted_general_volume_2018'] > 300000:
      return 1
    else:
      return 0

df_main['target'] = df_main.apply(calculate_target, axis=1)
df_main.target.sum()

df_main['predicted_general_volume_2018'].nlargest(40)

df_main_stores = df_main.copy()
df_main_stores = df_main_stores.drop(['cluster', 'credit'], axis=1)
df_main_stores.head(20)

df_main_stores.iloc[:, :-1]

"""Preparing main part - prediction model"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

_ = sns.pairplot(df_main_stores, hue='target')

df_main_stores.corr()

"""Random Forest"""

# Model RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Dividing data into training and test set
X = df_main_stores.iloc[:, :-1]
y = df_main_stores['target']

X_train, X_test, y_train, y_test = train_test_split(X, y,
                            random_state=42, test_size=0.2, stratify=y)

print(f'X_train shape {X_train.shape}')
print(f'y_train shape {y_train.shape}')
print(f'X_test shape {X_test.shape}')
print(f'y_test shape {y_test.shape}')
print(f'\nTest ratio: {len(X_test) / len(df_stores):.2f}')
print(f'\ntarget:\n{y.value_counts() / len(y)}')
print(f'\ny_train:\n{y_train.value_counts() / len(y_train)}')
print(f'\ny_test:\n{y_test.value_counts() / len(y_test)}')

model_RFC = RandomForestClassifier(random_state=42)

# Training the model on training data
model_RFC.fit(X_train, y_train)

# Prediction on test data
y_pred = model_RFC.predict(X_test)

# Model evaluation
accuracy = accuracy_score(y_test, y_pred)
accuracy

cm = confusion_matrix(y_test, y_pred)
cm

plot_confusion_matrix(cm)

print(classification_report(y_test, y_pred, target_names=['No Credit',
                                                          'Credit']))

# Calculating the importance of variables
feature_importance = model_RFC.feature_importances_

for i, importance in enumerate(feature_importance):
    print(f"Zmienna {i+1}: {importance}")

tn, fp, fn, tp = cm.ravel()
print(f'TN - True Negative: {tn}')
print(f'TN - False Positive: {fp}')
print(f'TN - False Negative: {fn}')
print(f'TN - True Positive: {tp}')

# Type I error - False Positive Rate
fpr = fp / (fp + tn)
fpr

# Type II error - False Negative Rate
fnr = fn / (fn + tp)
fnr

# Precision
precision = tp / (tp +fp)
precision

# Recall
recall = tp / (tp + fn)
recall

from sklearn.metrics import roc_curve
import plotly.graph_objects as go

fpr, tpr, thresh = roc_curve(y_test, y_pred, pos_label=1)
roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr})
roc

def plot_roc_curve(y_test, y_pred):
    """
    The function plots ROC Curve - binary classification model
    performance evaluation
    """
    from sklearn.metrics import roc_curve
    fpr, tpr, tresh = roc_curve(y_test, y_pred, pos_label=1)

    fig = go.Figure(data=[go.Scatter(x=roc['fpr'], y=roc['tpr'],
                                     line_color='red', name='ROC Curve'),
                            go.Scatter(x=[0, 1], y=[0, 1], mode='lines',
                                   line_dash='dash', line_color='navy')],
                    layout=go.Layout(xaxis_title='False Positive Rate',
                                    yaxis_title='True Positive Rate',
                                    title='ROC Curve',
                                    showlegend=False,
                                    width=800,
                                    height=400))
    fig.show()
plot_roc_curve(y_test, y_pred)

"""Cross-validation"""

from sklearn.model_selection import cross_val_score

scores = cross_val_score(estimator=model_RFC, X=X_train, y=y_train, cv=10)
scores

print(f'Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})')

